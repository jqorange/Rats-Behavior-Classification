{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Rats Behavior Classification","text":"<p>This project aims to build a robust multi-modal model for recognising laboratory rat behaviours. Existing approaches often suffer from low temporal resolution, limited label semantics and poor generalisation. Our goal is a 20\u00a0ms resolution classifier with F1 above 0.9 that can operate across sessions, days and even individual animals. To achieve this we combine video based DeepLabCut (DLC) features with IMU signals and train a semi-supervised model able to predict multiple actions per time step.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Common behaviour classification pipelines exhibit the following shortcomings:</p> <ol> <li>Most unsupervised methods such as B-SOiD or keypoint-moseq only cluster behaviours and the resulting groups are hard to interpret.</li> <li>Fully supervised models (e.g. DeepEthogram) require extensive manual labels yet still generalise poorly.</li> <li>They typically predict a single label at each instant while animals often perform multiple actions simultaneously.</li> <li>The temporal granularity is coarse (100\u2013200\u00a0ms) which misses fast movements.</li> <li>Tasks are simplified to a small set of behaviours leading to over\u2011fitting.</li> <li>Data are usually captured in constrained environments; models break down in the wild due to noise.</li> <li>Reliance on video alone fails when the subject leaves the frame or images are degraded.</li> </ol> <p>Our system tackles these issues by fusing DLC and IMU features. It uses deep dilated convolutions and a Transformer encoder to model local and long range context. Cross attention merges the two modalities so that when video is unreliable the IMU can compensate.</p>"},{"location":"#threestage-training-strategy","title":"Three\u2011Stage Training Strategy","text":"<p>Training proceeds in three distinct stages:</p> <ol> <li>Stage 1 \u2013 Session\u2011Aware Pretraining</li> <li>Each session is encoded separately via a session\u2011aware adapter.</li> <li>Unsupervised contrastive loss learns the internal structure of each session.</li> <li>Stage 2 \u2013 Session Alignment</li> <li>Encoders are frozen; a small adapter aligns sessions while preserving their structure with a contrastive loss.</li> <li>A supervised contrastive loss attracts samples with overlapping labels across sessions.</li> <li>Stage 3 \u2013 Fine\u2011Tuning</li> <li>All parameters are unfrozen.</li> <li>Samples with real labels and high\u2011confidence pseudo labels (\u22650.95 similarity) are trained with supervised contrastive learning using heavy Gaussian noise.</li> <li>An additional unsupervised contrastive loss mixes sequences from different sessions within the same batch.</li> <li>Samples are drawn inversely proportional to class counts and mixed up before computing the supervised contrastive loss.</li> </ol> <p>After these stages an MLP classifier is trained. High confidence predictions are added as new training samples to further improve performance.</p> <p>The full workflow is implemented in <code>utils/TrainPipline.py</code>. Checkpoints are saved under <code>checkpoints/</code> and can be resumed for continued training.</p> <p>Use the navigation bar to install the environment, prepare data and follow the quick start guide.</p>"},{"location":"Data_structure/","title":"Data Structure","text":"<p>Place your dataset under a common root folder with the following layout:</p> <p>''' IMU/ DLC/ sup_IMU/ sup_DLC/ sup_labels/ ''' All files are numpy arrays (<code>.npy</code>). Unsupervised recordings are stored as <code>samples_&lt;session&gt;.npy</code> inside <code>IMU/</code> and <code>DLC/</code>. Supervised segments and their labels follow <code>sup_IMU_&lt;session&gt;.npy</code>, <code>sup_DLC_&lt;session&gt;.npy</code> and <code>sup_labels_&lt;session&gt;.npy</code>.</p> <p>The list of session names used for training and testing is defined in <code>train.py</code>. Each session's arrays must have the same length for IMU and DLC so they can be aligned.</p>"},{"location":"install/","title":"Installation","text":"<p>The code requires Python 3.10 and PyTorch with CUDA support. The easiest way is to use conda:</p> <pre><code>conda create -n Behavior python=3.10\nconda activate Behavior\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\nconda install cudatoolkit=11.8\npip install plotly\n</code></pre> <p>After installation you can run the training scripts directly from the project root.</p>"},{"location":"quick_start/","title":"Quick Start","text":"<ol> <li>Install dependencies following Installation.</li> <li>Prepare the dataset as described in Data Structure.</li> <li>Train the encoders and classifier:</li> </ol> <pre><code>python train.py\n</code></pre> <p>Checkpoints are written to <code>checkpoints/</code> every few epochs.</p> <ol> <li>Generate latent representations for specific sessions:</li> </ol> <pre><code>python inference.py --data_path &lt;path_to_dataset&gt; --sessions F3D5_outdoor F3D6_outdoor\n</code></pre> <ol> <li>Visualise the embeddings using PCA:</li> </ol> <pre><code>python plot_embeddings.py --rep_dir representations --sessions F3D5_outdoor\n</code></pre> <ol> <li>Evaluate predictions on the labelled segments:</li> </ol> <pre><code>python evaluate.py --pred_dir predictions --label_path &lt;labels&gt;\n</code></pre>"},{"location":"usage/","title":"Usage Guide","text":"<p>All training and evaluation logic is wrapped by <code>utils/TrainPipline</code>. The entry point <code>train.py</code> creates a <code>TrainPipline</code> instance and runs the full three\u2011stage curriculum described on the home page.</p> <p>Most hyper parameters such as model dimensions and learning rates can be edited in <code>train.py</code> or passed to <code>FusionTrainer</code> through the pipeline.</p>"},{"location":"usage/#additional-scripts","title":"Additional Scripts","text":"<ul> <li>inference.py \u2013 generate representations from saved checkpoints.</li> <li>plot_embeddings.py \u2013 draw PCA plots for visual inspection.</li> <li>evaluate.py \u2013 evaluate predictions on CSV labels.</li> <li>multi_class_classifier.py / train_repr_mlp.py \u2013 self training utilities for representation level classifiers.</li> </ul> <p>Consult Files for a file\u2011by\u2011file description and Functions for API level documentation.</p>"},{"location":"usage/file/","title":"File Overview","text":""},{"location":"usage/file/#top-level-scripts","title":"Top level scripts","text":"<ul> <li>train.py \u2013 launches the full training pipeline defined in <code>utils/TrainPipline</code>.</li> <li>inference.py \u2013 loads checkpoints and produces latent representations for selected sessions.</li> <li>plot_embeddings.py \u2013 visualise representations with PCA.</li> <li>evaluate.py \u2013 compute metrics for prediction CSVs on labelled frames.</li> <li>multi_class_classifier.py / train_repr_mlp.py \u2013 utilities for self training a representation\u2011level MLP.</li> <li>multi_class_inference.py \u2013 run the saved MLP and output probabilities.</li> <li>predict_repr_mlp.py \u2013 inference using a MLP trained on representations.</li> </ul>"},{"location":"usage/file/#model-code-models","title":"Model code (<code>models/</code>)","text":"<ul> <li>dilated_conv.py \u2013 residual blocks of dilated 1D convolutions.</li> <li>encoder.py \u2013 single\u2011modal encoder combining domain adapters, dilated conv blocks and a Transformer.</li> <li>fusion.py \u2013 cross\u2011attention based fusion of IMU and DLC encoders.</li> <li>classifier.py \u2013 simple MLP classifier for sequence level labels.</li> <li>deep_mlp.py \u2013 deeper MLP used for representation level self training.</li> <li>domain_adapter.py \u2013 small modules inserted before encoders to align or tag sessions.</li> <li>masking.py \u2013 utilities to generate random temporal masks.</li> <li>losses.py \u2013 contrastive and reconstruction loss implementations.</li> </ul>"},{"location":"usage/file/#utilities-utils","title":"Utilities (<code>utils/</code>)","text":"<ul> <li>data_loader.py \u2013 loads IMU/DLC numpy files and their supervised counterparts.</li> <li>trainer.py \u2013 core optimisation logic with contrastive and MLP phases.</li> <li>TrainPipline.py \u2013 high level wrapper orchestrating data loading, model setup and evaluation.</li> <li>tools.py \u2013 helper functions for sequence slicing.</li> </ul>"},{"location":"usage/function/","title":"Function Reference","text":"<p>Below is a high level summary of the public functions and class methods found in the repository. They are grouped by file for easy lookup.</p>"},{"location":"usage/function/#evaluatepy","title":"evaluate.py","text":"<ul> <li>load_valid_segments \u2013 read <code>results.txt</code> and return a mapping of valid label segments for each session.</li> <li>evaluate_predictions \u2013 compute accuracy, precision, recall and F1 for each label column.</li> <li>load_labels \u2013 read ground truth CSV labels for one session.</li> <li>load_predictions \u2013 slice prediction CSVs using segment indices.</li> <li>main \u2013 command line entry for batch evaluation of prediction files.</li> </ul>"},{"location":"usage/function/#inferencepy","title":"inference.py","text":"<ul> <li>latest_checkpoint \u2013 locate the most recent encoder checkpoint.</li> <li>main \u2013 generate representations for one or more sessions.</li> </ul>"},{"location":"usage/function/#modelsclassifierpy","title":"models/classifier.py","text":"<ul> <li>MLPClassifier.init \u2013 build a small two layer MLP.</li> <li>MLPClassifier.forward \u2013 apply the classifier to an input tensor.</li> </ul>"},{"location":"usage/function/#modelsdeep_mlppy","title":"models/deep_mlp.py","text":"<ul> <li>DeepMLPClassifier.init \u2013 construct a deeper MLP with configurable hidden sizes.</li> <li>DeepMLPClassifier.forward \u2013 run the classifier.</li> </ul>"},{"location":"usage/function/#modelsdilated_convpy","title":"models/dilated_conv.py","text":"<ul> <li>SamePadConv.init \u2013 convolution with automatic same padding.</li> <li>SamePadConv.forward \u2013 run the convolution and trim extra padding.</li> <li>ConvBlock.init \u2013 residual block of two convolutions.</li> <li>ConvBlock.forward \u2013 forward pass with GELU activations.</li> <li>DilatedConvEncoder.init \u2013 stack multiple residual blocks with increasing dilation.</li> <li>DilatedConvEncoder.forward \u2013 encode a sequence.</li> </ul>"},{"location":"usage/function/#modelsencoderpy","title":"models/encoder.py","text":"<ul> <li>Encoder.init \u2013 linear projection, dilated conv stack and dual GRU heads.</li> <li>Encoder.forward \u2013 encode a sequence with optional masking and return reconstruction logits.</li> </ul>"},{"location":"usage/function/#modelsfusionpy","title":"models/fusion.py","text":"<ul> <li>EncoderFusion.init \u2013 create two encoders, per-sample cross-attention and projection head.</li> <li>EncoderFusion.forward \u2013 return a <code>FusionOutput</code> with fused, cross-modal and reconstruction tensors.</li> </ul>"},{"location":"usage/function/#train_newpy","title":"train_new.py","text":"<ul> <li>TwoStageTrainer \u2013 two-stage curriculum alternating unsupervised and supervised batches.</li> <li>main \u2013 command line entry for the streamlined training pipeline.</li> </ul>"},{"location":"usage/function/#modelslossespy","title":"models/losses.py","text":"<ul> <li>hierarchical_contrastive_loss \u2013 instance and temporal contrastive loss across multiple scales.</li> <li>instance_contrastive_loss \u2013 contrast between samples within the same batch.</li> <li>temporal_contrastive_loss \u2013 contrast across time within each sequence.</li> <li>multilabel_supcon_loss_bt \u2013 multi\u2011label supervised contrastive loss with pooling.</li> <li>compute_contrastive_losses \u2013 dispatch supervised or unsupervised contrastive losses depending on stage.</li> <li>CenterLoss.init \u2013 maintain class centroids.</li> <li>CenterLoss.forward \u2013 compute distance from features to class centers.</li> <li>UncertaintyWeighting.init \u2013 learn weights for combining multiple losses.</li> <li>UncertaintyWeighting.forward \u2013 apply uncertainty weighting.</li> </ul>"},{"location":"usage/function/#modelsmaskingpy","title":"models/masking.py","text":"<ul> <li>generate_continuous_mask \u2013 drop contiguous blocks of frames.</li> <li>generate_binomial_mask \u2013 sample frames independently using a Bernoulli distribution.</li> </ul>"},{"location":"usage/function/#multi_class_classifierpy","title":"multi_class_classifier.py","text":"<ul> <li>load_valid_segments \u2013 parse <code>results.txt</code> for valid label regions.</li> <li>load_session_data \u2013 load features and labels for a session.</li> <li>load_unlabeled_data \u2013 return data outside labelled segments for self training.</li> <li>build_datasets \u2013 assemble training, test and unlabeled datasets.</li> <li>evaluate \u2013 compute accuracy and F1 for a data loader.</li> <li>pseudo_label \u2013 generate pseudo labels from high confidence predictions.</li> <li>self_training \u2013 iterative pseudo label training loop.</li> <li>main \u2013 command line interface for self training.</li> </ul>"},{"location":"usage/function/#multi_class_inferencepy","title":"multi_class_inference.py","text":"<ul> <li>load_representation \u2013 load a representation file from disk.</li> <li>run_inference \u2013 run a saved MLP on one session and save predictions.</li> <li>main \u2013 script entry point.</li> </ul>"},{"location":"usage/function/#plot_embeddingspy","title":"plot_embeddings.py","text":"<ul> <li>load_channel_names \u2013 read channel names for plotting.</li> <li>compute_cluster_metrics \u2013 evaluate cluster purity in PCA space.</li> <li>main \u2013 draw PCA plot from representation files.</li> </ul>"},{"location":"usage/function/#predict_repr_mlppy","title":"predict_repr_mlp.py","text":"<ul> <li>load_model \u2013 load a representation\u2011level MLP.</li> <li>main \u2013 perform inference on multiple sessions.</li> </ul>"},{"location":"usage/function/#trainpy","title":"train.py","text":"<ul> <li>main \u2013 create a <code>TrainPipline</code> and run the training workflow.</li> </ul>"},{"location":"usage/function/#train_repr_mlppy","title":"train_repr_mlp.py","text":"<ul> <li>load_valid_segments \u2013 parse <code>results.txt</code> for valid label regions.</li> <li>load_session_data \u2013 load representations and labels for supervised training.</li> <li>load_unlabeled_data \u2013 load unlabeled representations.</li> <li>build_datasets \u2013 prepare training and test sets with optional unlabeled data.</li> <li>evaluate \u2013 compute accuracy and F1 of the deep MLP.</li> <li>pseudo_label \u2013 select high confidence samples as pseudo labels.</li> <li>self_training \u2013 multi\u2011step self training loop.</li> <li>main \u2013 command line entry.</li> </ul>"},{"location":"usage/function/#utilstrainpiplinepy","title":"utils/TrainPipline.py","text":"<ul> <li>TrainPipline.init \u2013 set paths, sessions and basic parameters.</li> <li>TrainPipline._setup_device \u2013 choose CPU or CUDA device.</li> <li>TrainPipline.load_and_prepare_data \u2013 load, truncate and align all sessions.</li> <li>prepare_session \u2013 inner helper used during loading.</li> <li>TrainPipline.initialize_trainer \u2013 create the <code>FusionTrainer</code> with given hyper parameters.</li> <li>TrainPipline.test_model_components \u2013 sanity check that the model runs on a few samples.</li> <li>TrainPipline.train_model \u2013 run the three\u2011stage training using the trainer.</li> <li>TrainPipline.evaluate_model \u2013 evaluate the trained model on the test set.</li> <li>TrainPipline.run_full_pipeline \u2013 convenience function executing the entire workflow.</li> </ul>"},{"location":"usage/function/#utilsdata_loaderpy","title":"utils/data_loader.py","text":"<ul> <li>DataLoader.init \u2013 configure paths for IMU, DLC and label files.</li> <li>DataLoader.load_original_data \u2013 read unsupervised IMU and DLC arrays.</li> <li>DataLoader.load_supervised_data \u2013 read supervised segments and labels.</li> <li>DataLoader.load_all_data \u2013 call both loading functions and return a summary.</li> <li>DataLoader.get_data_summary \u2013 return dictionaries of all loaded arrays.</li> <li>DataLoader.get_session_data \u2013 fetch data for a single session.</li> <li>DataLoader.print_data_info \u2013 print statistics about every session.</li> </ul>"},{"location":"usage/function/#utilstoolspy","title":"utils/tools.py","text":"<ul> <li>take_per_row \u2013 slice a batch of sequences starting at different offsets.</li> <li>take_per_row_safe \u2013 safe version that pads beyond sequence ends.</li> <li>take_per_row_vectorized \u2013 vectorised slicing implementation.</li> <li>test_take_per_row \u2013 demonstration of the slicing functions.</li> </ul>"},{"location":"usage/function/#utilstrainerpy","title":"utils/trainer.py","text":"<ul> <li>FusionTrainer.init \u2013 build encoders, classifier and optimisers.</li> <li>train_contrastive_phase \u2013 run a contrastive learning stage.</li> <li>train_stage3 \u2013 stage 3 training combining supervised and unsupervised losses with KL distribution alignment.</li> <li>train_contrastive_multi_session \u2013 unsupervised training mixing sessions per batch.</li> <li>train_mlp_phase \u2013 supervised MLP training after the encoders are frozen.</li> <li>fit \u2013 execute the full three\u2011stage curriculum.</li> <li>init_stage2 \u2013 load adapters and freeze encoders for stage 2.</li> <li>init_stage3 \u2013 prepare the model for stage 3 fine\u2011tuning.</li> <li>encode \u2013 encode IMU and DLC sequences without classification.</li> <li>encode_state1 \u2013 encode using the session\u2011aware adapters from stage 1.</li> <li>predict \u2013 run the MLP classifier on new data.</li> <li>save \u2013 save encoder and classifier checkpoints.</li> <li>load \u2013 load checkpoints.</li> <li>load_stage2 \u2013 load only the encoder for stage 2 inference.</li> </ul>"}]}